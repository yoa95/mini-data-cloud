# Mini Data Cloud

A containerized distributed data processing system that teaches core cloud database concepts through hands-on implementation.

## ğŸ¯ Project Goals

Build a simplified version of systems like **Databricks**, **Snowflake**, or **Amazon Redshift** to understand:

- **Control Plane vs Data Plane** architecture patterns
- **Distributed query planning** and execution
- **Modern data formats** (Parquet, Iceberg) and processing engines (Arrow, Calcite)
- **Container orchestration** and service communication
- **ACID transactions** and schema evolution
- **Fault tolerance** and observability in distributed systems

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Control Plane (Java)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ API Gateway â”‚ â”‚   Metadata  â”‚ â”‚    Query Planner        â”‚ â”‚
â”‚  â”‚ Spring Boot â”‚ â”‚   Service   â”‚ â”‚  Apache Calcite         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚ gRPC
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Plane (Java)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Worker 1   â”‚ â”‚  Worker 2   â”‚ â”‚      Worker N           â”‚ â”‚
â”‚  â”‚ Arrow Java  â”‚ â”‚ Arrow Java  â”‚ â”‚    Arrow Java           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Storage Layer                             â”‚
â”‚           Iceberg Tables + Parquet Files                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

```bash
# Required tools
java --version    # Java 17+
mvn --version     # Maven 3.8+
docker --version  # Docker 20.10+
```

### Build and Run

```bash
# Clone and build
git clone <repository-url>
cd mini-data-cloud
mvn clean install

# Start development environment
docker-compose up -d

# Access web UI
open http://localhost:8080
```

### Try It Out

```bash
# 1. Load your personal CSV data
curl -X POST http://localhost:8080/api/v1/tables/transactions/load \
  -H "Content-Type: application/json" \
  -d '{"file_path": "/data/bank_transactions.csv"}'

# 2. Run analytics queries
curl -X POST http://localhost:8080/api/v1/queries \
  -H "Content-Type: application/json" \
  -d '{"sql": "SELECT category, SUM(amount) FROM transactions GROUP BY category"}'

# 3. Check results
curl http://localhost:8080/api/v1/queries/{query_id}
```

## ğŸ› ï¸ Technology Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Control Plane** | Java + Spring Boot | API gateway, query planning, orchestration |
| **Data Plane** | Java + Arrow | High-performance query execution |
| **SQL Engine** | Apache Calcite | SQL parsing and optimization |
| **Storage** | Parquet + Iceberg | Columnar storage with ACID transactions |
| **Communication** | gRPC + Arrow Flight | Service coordination and data exchange |
| **Orchestration** | Docker + Docker Compose | Container management |
| **Observability** | Prometheus + OpenTelemetry | Metrics and distributed tracing |

## ğŸ“š Learning Path

### Phase 1: Single-Node SQL Engine (Weeks 1-2)
**What you'll learn**: SQL parsing, columnar processing, Parquet I/O

- âœ… Load personal CSV files and convert to Parquet
- âœ… Execute SQL queries using Apache Calcite + Arrow Java
- âœ… Measure performance characteristics of columnar processing

**Example**: Analyze your bank transactions or fitness data with SQL

### Phase 2: Distributed Architecture (Weeks 3-4)
**What you'll learn**: Control/data plane separation, gRPC communication, distributed execution

- âœ… Split into control plane (planning) and data plane (execution)
- âœ… Distribute queries across multiple worker containers
- âœ… Handle worker failures and implement retry logic

**Example**: Scale from 1 to 3 workers and measure query speedup

### Phase 3: Production Features (Weeks 5-6)
**What you'll learn**: ACID transactions, schema evolution, observability

- âœ… Add Iceberg for time travel queries: "Show data as of yesterday"
- âœ… Implement schema evolution: Add columns without breaking existing queries
- âœ… Add security (mTLS), monitoring (Prometheus), and auto-scaling

**Example**: Evolve your data schema and query historical snapshots

## ğŸ“ Key Concepts You'll Master

### Distributed Systems Patterns
- **Control Plane vs Data Plane**: Separation of coordination and execution
- **Query Planning**: Converting SQL to distributed execution plans
- **Fault Tolerance**: Handling worker failures and network partitions
- **Load Balancing**: Distributing work across available resources

### Modern Data Engineering
- **Columnar Storage**: Why Parquet is efficient for analytics
- **Vectorized Processing**: How Arrow accelerates query execution
- **ACID Transactions**: Ensuring data consistency in distributed systems
- **Schema Evolution**: Safely changing data structures over time

### Cloud Architecture
- **Microservices**: Independent, scalable service components
- **Container Orchestration**: Managing distributed applications
- **Observability**: Monitoring, logging, and tracing distributed systems
- **Security**: Authentication, authorization, and encrypted communication

## ğŸ”¥ Real-World Use Cases

### Personal Analytics Playground
```sql
-- Analyze your spending patterns
SELECT 
    category,
    EXTRACT(MONTH FROM date) as month,
    SUM(amount) as total_spent
FROM bank_transactions 
WHERE date >= '2024-01-01'
GROUP BY category, EXTRACT(MONTH FROM date)
ORDER BY total_spent DESC;
```

### Time Travel Analysis
```sql
-- Compare data from different time periods
SELECT COUNT(*) as row_count 
FROM fitness_data 
FOR SYSTEM_TIME AS OF '2024-01-01';
```

### Schema Evolution Demo
```sql
-- Add new column and query across old/new data
ALTER TABLE user_events ADD COLUMN device_type VARCHAR(50);

-- Query works across all data versions
SELECT device_type, COUNT(*) 
FROM user_events 
GROUP BY device_type;
```

## ğŸ“Š Performance Benchmarks

| Dataset Size | Single Node | 3 Workers | Speedup |
|--------------|-------------|-----------|---------|
| 100K rows    | 0.5s        | 0.3s      | 1.7x    |
| 1M rows      | 3.2s        | 1.1s      | 2.9x    |
| 10M rows     | 28s         | 9.8s      | 2.9x    |

*Benchmarks run on TPC-H lite dataset with standard aggregation queries*

## ğŸƒâ€â™‚ï¸ Development Workflow

### Project Structure
```
mini-data-cloud/
â”œâ”€â”€ minicloud-control-plane/    # Query planning and orchestration
â”œâ”€â”€ minicloud-worker/           # Distributed query execution  
â”œâ”€â”€ minicloud-common/           # Shared models and utilities
â”œâ”€â”€ proto/                      # gRPC service definitions
â”œâ”€â”€ tools/                      # Testing and deployment tools
â””â”€â”€ docker-compose.yml          # Development environment
```

### Running Tests
```bash
# Unit tests
mvn test

# Integration tests with Testcontainers
mvn test -Dtest=*IntegrationTest

# Performance benchmarks
mvn test -Dtest=*BenchmarkTest
```

### Monitoring and Debugging
```bash
# View metrics dashboard
open http://localhost:3000  # Grafana

# Check distributed traces
open http://localhost:16686  # Jaeger

# Monitor JVM performance
jvisualvm --jdkhome $JAVA_HOME
```

## ğŸ¤ Contributing

This is a learning project! Feel free to:

- **Experiment** with different query optimization strategies
- **Add features** like more SQL operators or storage formats
- **Optimize performance** with better algorithms or JVM tuning
- **Extend observability** with custom metrics and dashboards

## ğŸ“– Further Reading

### Distributed Systems
- [Designing Data-Intensive Applications](https://dataintensive.net/) by Martin Kleppmann
- [Database Internals](https://www.databass.dev/) by Alex Petrov

### Modern Data Formats
- [Apache Iceberg Documentation](https://iceberg.apache.org/)
- [Apache Arrow Documentation](https://arrow.apache.org/)
- [Apache Parquet Documentation](https://parquet.apache.org/)

### Query Engines
- [Apache Calcite Documentation](https://calcite.apache.org/)
- [Presto/Trino Architecture](https://trino.io/docs/current/overview/concepts.html)

## ğŸ“„ License

MIT License - feel free to use this for learning, teaching, or building upon!

---

**Ready to build your own data cloud?** Start with Phase 1 and work your way up to a production-ready distributed system! ğŸš€